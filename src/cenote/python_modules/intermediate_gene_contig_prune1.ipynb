{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a gene table with:\n",
    "contig, contig_length, gene_name, gene_start, gene_stop, gene_orient, Evidence_source, evidence_acession, evidence_description, virus_score(or category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "from itertools import tee\n",
    "import csv\n",
    "\n",
    "import statistics\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import math\n",
    "import collections\n",
    "import bisect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_table = \"/Users/u241374/mike_tisza/sandbox/test_ct2_0802a/ct2_tmp/hallmark_contigs_terminal_repeat_summary.tsv\"\n",
    "\n",
    "phan_tab_directory = \"/Users/u241374/mike_tisza/sandbox/test_ct2_0802a/ct2_tmp/reORF/phan_split\"\n",
    "\n",
    "prod_tab_directory = \"/Users/u241374/mike_tisza/sandbox/test_ct2_0802a/ct2_tmp/reORF/prod_split\"\n",
    "\n",
    "first_pyhmmer_table = \"/Users/u241374/mike_tisza/sandbox/test_ct2_0802a/ct2_tmp/reORF_pyhmmer/pyhmmer_report_AAs.tsv\"\n",
    "\n",
    "second_pyhmmer_table = \"/Users/u241374/mike_tisza/sandbox/test_ct2_0802a/ct2_tmp/second_reORF_pyhmmer/pyhmmer_report_AAs.tsv\"\n",
    "\n",
    "mmseqs_CDD_table = \"/Users/u241374/mike_tisza/sandbox/test_ct2_0802a/ct2_tmp/third_reORF_combined/summary_no2_AAs_vs_CDD.besthit.tsv\"\n",
    "\n",
    "viral_cdds_list = \"/Users/u241374/mike_tisza/cmmr_repos/Cenote-Taker2/viral_cdds_and_pfams_191028.txt\"\n",
    "\n",
    "out_dir = \"/Users/u241374/mike_tisza/sandbox/figures3\"\n",
    "\n",
    "if not os.path.isdir(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "\n",
    "    phan_files = glob.glob(os.path.join(phan_tab_directory, \"*.bed\"))\n",
    "\n",
    "    df_from_each_phan = (pd.read_csv(phan, sep = \"\\t\", header = None,\n",
    "                                    names = [\"contig\", \"gene_start\", \"gene_stop\", \"gene_name\", \n",
    "                                            \"gene_score\", \"gene_orient\"])\n",
    "                                    for phan in phan_files)\n",
    "    phan_gene_df = pd.concat(df_from_each_phan, ignore_index=True)\n",
    "\n",
    "    phan_gene_df = phan_gene_df.drop(\"gene_score\", axis = 1)\n",
    "\n",
    "except:\n",
    "    print(\"no phanotate tables\")\n",
    "    phan_gene_df = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contig</th>\n",
       "      <th>gene_start</th>\n",
       "      <th>gene_stop</th>\n",
       "      <th>attribute</th>\n",
       "      <th>gene_orient</th>\n",
       "      <th>semi_pos</th>\n",
       "      <th>gene_IDstr</th>\n",
       "      <th>under_pos</th>\n",
       "      <th>gene_num</th>\n",
       "      <th>gene_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_ct2_0802a3</td>\n",
       "      <td>2</td>\n",
       "      <td>334</td>\n",
       "      <td>ID=1_1;partial=10;start_type=ATG;rbs_motif=Non...</td>\n",
       "      <td>-</td>\n",
       "      <td>6</td>\n",
       "      <td>ID=1_1</td>\n",
       "      <td>4</td>\n",
       "      <td>_1</td>\n",
       "      <td>test_ct2_0802a3_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_ct2_0802a3</td>\n",
       "      <td>339</td>\n",
       "      <td>959</td>\n",
       "      <td>ID=1_2;partial=00;start_type=ATG;rbs_motif=Non...</td>\n",
       "      <td>+</td>\n",
       "      <td>6</td>\n",
       "      <td>ID=1_2</td>\n",
       "      <td>4</td>\n",
       "      <td>_2</td>\n",
       "      <td>test_ct2_0802a3_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_ct2_0802a3</td>\n",
       "      <td>964</td>\n",
       "      <td>1257</td>\n",
       "      <td>ID=1_3;partial=00;start_type=ATG;rbs_motif=GGA...</td>\n",
       "      <td>+</td>\n",
       "      <td>6</td>\n",
       "      <td>ID=1_3</td>\n",
       "      <td>4</td>\n",
       "      <td>_3</td>\n",
       "      <td>test_ct2_0802a3_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_ct2_0802a3</td>\n",
       "      <td>1295</td>\n",
       "      <td>1774</td>\n",
       "      <td>ID=1_4;partial=00;start_type=ATG;rbs_motif=Non...</td>\n",
       "      <td>-</td>\n",
       "      <td>6</td>\n",
       "      <td>ID=1_4</td>\n",
       "      <td>4</td>\n",
       "      <td>_4</td>\n",
       "      <td>test_ct2_0802a3_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_ct2_0802a3</td>\n",
       "      <td>2249</td>\n",
       "      <td>3514</td>\n",
       "      <td>ID=1_5;partial=00;start_type=ATG;rbs_motif=Non...</td>\n",
       "      <td>-</td>\n",
       "      <td>6</td>\n",
       "      <td>ID=1_5</td>\n",
       "      <td>4</td>\n",
       "      <td>_5</td>\n",
       "      <td>test_ct2_0802a3_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            contig  gene_start  gene_stop  \\\n",
       "0  test_ct2_0802a3           2        334   \n",
       "1  test_ct2_0802a3         339        959   \n",
       "2  test_ct2_0802a3         964       1257   \n",
       "3  test_ct2_0802a3        1295       1774   \n",
       "4  test_ct2_0802a3        2249       3514   \n",
       "\n",
       "                                           attribute gene_orient  semi_pos  \\\n",
       "0  ID=1_1;partial=10;start_type=ATG;rbs_motif=Non...           -         6   \n",
       "1  ID=1_2;partial=00;start_type=ATG;rbs_motif=Non...           +         6   \n",
       "2  ID=1_3;partial=00;start_type=ATG;rbs_motif=GGA...           +         6   \n",
       "3  ID=1_4;partial=00;start_type=ATG;rbs_motif=Non...           -         6   \n",
       "4  ID=1_5;partial=00;start_type=ATG;rbs_motif=Non...           -         6   \n",
       "\n",
       "  gene_IDstr  under_pos gene_num          gene_name  \n",
       "0     ID=1_1          4       _1  test_ct2_0802a3_1  \n",
       "1     ID=1_2          4       _2  test_ct2_0802a3_2  \n",
       "2     ID=1_3          4       _3  test_ct2_0802a3_3  \n",
       "3     ID=1_4          4       _4  test_ct2_0802a3_4  \n",
       "4     ID=1_5          4       _5  test_ct2_0802a3_5  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    prod_files = glob.glob(os.path.join(prod_tab_directory, \"*.gff\"))\n",
    "\n",
    "    df_from_each_prod = (pd.read_csv(prod, sep = \"\\t\", header = None, comment='#',\n",
    "                                    names = [\"contig\", \"gene_caller\", \"feature_type\", \"gene_start\", \n",
    "                                            \"gene_stop\", \"score\", \"gene_orient\", \"frame\", \"attribute\"])\n",
    "                                    for prod in prod_files)\n",
    "    prod_gene_df = pd.concat(df_from_each_prod, ignore_index=True)\n",
    "\n",
    "    prod_gene_df = prod_gene_df[[\"contig\", \"gene_start\", \"gene_stop\", \"attribute\", \"gene_orient\"]]\n",
    "\n",
    "    prod_gene_df[\"semi_pos\"] = prod_gene_df[\"attribute\"].str.find(\";\")\n",
    "    prod_gene_df[\"gene_IDstr\"] = prod_gene_df.apply(\n",
    "        lambda x: x[\"attribute\"][0:x[\"semi_pos\"]], axis = 1)\n",
    "    \n",
    "    prod_gene_df[\"under_pos\"] = prod_gene_df[\"gene_IDstr\"].str.rfind(\"_\")\n",
    "    prod_gene_df[\"gene_num\"] = prod_gene_df.apply(\n",
    "        lambda x: x[\"gene_IDstr\"][x[\"under_pos\"]:len(x[\"gene_IDstr\"])], axis = 1)\n",
    "    \n",
    "    prod_gene_df[\"gene_name\"] = prod_gene_df[\"contig\"] + prod_gene_df[\"gene_num\"]\n",
    "\n",
    "\n",
    "except:\n",
    "    print(\"no prodigal tables\")\n",
    "    prod_gene_df = pd.DataFrame()\n",
    "\n",
    "prod_gene_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "both\n"
     ]
    }
   ],
   "source": [
    "if not phan_gene_df.empty and not prod_gene_df.empty:\n",
    "    print(\"both\")\n",
    "    both_list = [phan_gene_df, prod_gene_df]\n",
    "    just_gene_df = pd.concat(both_list, ignore_index=True)\n",
    "elif not phan_gene_df.empty:\n",
    "    print(\"phan\")\n",
    "    just_gene_df = phan_gene_df\n",
    "elif not prod_gene_df.empty:\n",
    "    print(\"prod\")\n",
    "    just_gene_df = prod_gene_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    length_df = pd.read_csv(repeat_table, sep = \"\\t\")[['contig', 'out_length_contig', 'dtr_seq']]\n",
    "\n",
    "    length_df = length_df.rename(columns={\"out_length_contig\": \"contig_length\"})\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    basal_df = just_gene_df.merge(length_df, on = \"contig\", how = \"left\")\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_pyh_df = pd.read_csv(first_pyhmmer_table, sep = \"\\t\")[['ORFquery', 'target']]\n",
    "\n",
    "first_pyh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_name, Evidence_source, evidence_acession, evidence_description, virus_score(or category)\n",
    "\n",
    "try:\n",
    "    first_pyh_df = pd.read_csv(first_pyhmmer_table, sep = \"\\t\")[['ORFquery', 'target']]\n",
    "\n",
    "\n",
    "\n",
    "    first_pyh_df[\"gene_name\"] = first_pyh_df[\"ORFquery\"]\n",
    "\n",
    "    first_pyh_df[\"slash_pos\"] = first_pyh_df[\"target\"].str.find(\"/\")\n",
    "    first_pyh_df[\"fdash_pos\"] = first_pyh_df[\"target\"].str.find(\"-\")\n",
    "\n",
    "\n",
    "    first_pyh_df[\"evidence_acession\"] = first_pyh_df.apply(\n",
    "        lambda x: x[\"target\"][x[\"slash_pos\"]+1:x[\"fdash_pos\"]], axis = 1)\n",
    "\n",
    "    first_pyh_df[\"evidence_description\"] = first_pyh_df.apply(lambda x: x[\"target\"][x[\"fdash_pos\"]+1:], axis = 1)\n",
    "\n",
    "    first_pyh_df = first_pyh_df[['gene_name', 'evidence_acession', 'evidence_description']]\n",
    "\n",
    "    first_pyh_df['Evidence_source'] = 'hallmark_hmm'\n",
    "\n",
    "    first_pyh_df['vscore_category'] = 'common_virus'\n",
    "\n",
    "except:\n",
    "    print(\"nope\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    second_pyh_df = pd.read_csv(second_pyhmmer_table, sep = \"\\t\")[['ORFquery', 'target']]\n",
    "\n",
    "\n",
    "    second_pyh_df[\"lpar_pos\"] = second_pyh_df[\"ORFquery\"].str.find(\"(\")\n",
    "\n",
    "    second_pyh_df[\"gene_name\"] = second_pyh_df.apply(lambda x: x[\"ORFquery\"][0:x[\"lpar_pos\"]], axis = 1)\n",
    "\n",
    "    second_pyh_df[\"slash_pos\"] = second_pyh_df[\"target\"].str.find(\"/\")\n",
    "    second_pyh_df[\"fdash_pos\"] = second_pyh_df[\"target\"].str.find(\"-\")\n",
    "\n",
    "\n",
    "    second_pyh_df[\"evidence_acession\"] = second_pyh_df.apply(\n",
    "        lambda x: x[\"target\"][x[\"slash_pos\"]+1:x[\"fdash_pos\"]], axis = 1)\n",
    "\n",
    "    second_pyh_df[\"evidence_description\"] = second_pyh_df.apply(lambda x: x[\"target\"][x[\"fdash_pos\"]+1:], axis = 1)\n",
    "\n",
    "    second_pyh_df = second_pyh_df[['gene_name', 'evidence_acession', 'evidence_description']]\n",
    "\n",
    "    second_pyh_df['Evidence_source'] = 'common_virus_hmm'\n",
    "\n",
    "    second_pyh_df['vscore_category'] = 'common_virus'\n",
    "\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_name</th>\n",
       "      <th>evidence_acession</th>\n",
       "      <th>evidence_description</th>\n",
       "      <th>Evidence_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ct2_b_caccae_b1_10</td>\n",
       "      <td>COG2239</td>\n",
       "      <td>Mg/Co/Ni transporter MgtE (contains CBS domain...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ct2_b_caccae_b1_1001</td>\n",
       "      <td>pfam09561</td>\n",
       "      <td>HpaII restriction endonuclease. This family in...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ct2_b_caccae_b1_1002</td>\n",
       "      <td>cd02136</td>\n",
       "      <td>nitroreductase similar to Mycobacterium smegma...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ct2_b_caccae_b1_1003</td>\n",
       "      <td>TIGR00461</td>\n",
       "      <td>glycine dehydrogenase (decarboxylating). This ...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ct2_b_caccae_b</td>\n",
       "      <td>cd06262</td>\n",
       "      <td>mainly hydrolytic enzymes and related proteins...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              gene_name evidence_acession  \\\n",
       "0    ct2_b_caccae_b1_10           COG2239   \n",
       "1  ct2_b_caccae_b1_1001         pfam09561   \n",
       "2  ct2_b_caccae_b1_1002           cd02136   \n",
       "3  ct2_b_caccae_b1_1003         TIGR00461   \n",
       "4        ct2_b_caccae_b           cd06262   \n",
       "\n",
       "                                evidence_description Evidence_source  \n",
       "0  Mg/Co/Ni transporter MgtE (contains CBS domain...      mmseqs_cdd  \n",
       "1  HpaII restriction endonuclease. This family in...      mmseqs_cdd  \n",
       "2  nitroreductase similar to Mycobacterium smegma...      mmseqs_cdd  \n",
       "3  glycine dehydrogenase (decarboxylating). This ...      mmseqs_cdd  \n",
       "4  mainly hydrolytic enzymes and related proteins...      mmseqs_cdd  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdd_df = pd.read_csv(mmseqs_CDD_table, sep = \"\\t\")[['query', 'target', 'description']]\n",
    "\n",
    "cdd_df['lpar_pos'] = np.where(cdd_df['query']\n",
    "                               .str.contains(\"\\(\"), \n",
    "                               cdd_df[\"query\"].str.find(\"(\"), len(cdd_df['query']))\n",
    "\n",
    "#cdd_df[\"lpar_pos\"] = cdd_df[\"query\"].str.find(\"(\")\n",
    "\n",
    "cdd_df[\"gene_name\"] = cdd_df.apply(lambda x: x[\"query\"][0:x[\"lpar_pos\"]], axis = 1)\n",
    "\n",
    "cdd_df = cdd_df[['gene_name', 'target', 'description']]\n",
    "\n",
    "cdd_df = cdd_df.rename(columns={\"target\": \"evidence_acession\", \"description\" : \"evidence_description\"})\n",
    "\n",
    "cdd_df['Evidence_source'] = 'mmseqs_cdd'\n",
    "\n",
    "#cdd_df['vscore_category'] = 'nonviral_gene'\n",
    "\n",
    "#cdd_df['vscore_category'] = np.where(cdd_df['target']\n",
    "#                               .str.contains(\n",
    "#                                   \"PHA0\",\n",
    "#                                   case = False), 'common_virus', cdd_df['vscore_category'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cdd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virlist_df = pd.read_csv(viral_cdds_list, header = None, names = [\"evidence_acession\"])\n",
    "\n",
    "virlist_df['vscore_category'] = 'common_virus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_cdd_df = cdd_df.merge(virlist_df, on = \"evidence_acession\", how = \"left\")\n",
    "\n",
    "comb_cdd_df['vscore_category'] = np.where(comb_cdd_df['evidence_acession']\n",
    "                               .str.contains(\n",
    "                                   \"PHA0\",\n",
    "                                   case = False), 'common_virus', comb_cdd_df['vscore_category'])\n",
    "\n",
    "comb_cdd_df['vscore_category'] = np.where(comb_cdd_df['vscore_category'].isna(), 'nonviral_gene',\n",
    "                                          comb_cdd_df['vscore_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ann_list = []\n",
    "\n",
    "if not first_pyh_df.empty:\n",
    "    gene_ann_list.append(first_pyh_df)\n",
    "\n",
    "if not second_pyh_df.empty:\n",
    "    gene_ann_list.append(second_pyh_df)\n",
    "\n",
    "if not comb_cdd_df.empty:\n",
    "    gene_ann_list.append(comb_cdd_df)\n",
    "\n",
    "try:\n",
    "    gene_ann_df = pd.concat(gene_ann_list, ignore_index=True)\n",
    "\n",
    "    contig_gene_df = basal_df.merge(gene_ann_df, on = \"gene_name\", how = \"left\")\n",
    "\n",
    "    contig_gene_df['vscore_category'] = np.where(contig_gene_df['vscore_category'].isna(), 'hypothetical_protein',\n",
    "                                          contig_gene_df['vscore_category'])\n",
    "    \n",
    "    contig_gene_df['evidence_description'] = np.where(contig_gene_df['evidence_description'].isna(), 'hypothetical protein',\n",
    "                                          contig_gene_df['evidence_description'])\n",
    "\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "grouped_df = contig_gene_df.query(\"contig_length >= 10000\").groupby('contig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prune(name, group, out_dir1):\n",
    "\n",
    "    ## function takes the name and data from the grouped pandas dataframe\n",
    "    ## grouped_df\n",
    "    ## and the output directory\n",
    "\n",
    "    ####define scoring scheme and important variables\n",
    "    id_list = ['common_virus','hypothetical_protein','nonviral_gene','intergenic']\n",
    "    score_list = [10,5,-3,0]\n",
    "\n",
    "    keys = id_list\n",
    "    values = score_list\n",
    "    domain_dictionary = dict(zip(keys,values))\n",
    "\n",
    "    threshold = 0\n",
    "    window = 5000\n",
    "    sliiiiiide_to_the_right = 50\n",
    "\n",
    "    count=0\n",
    "    count_start=-sliiiiiide_to_the_right\n",
    "\n",
    "    contig_length1 = group['contig_length'].agg(pd.Series.mode)\n",
    "    ####\n",
    "\n",
    "    ## make the score list based on annotations\n",
    "    vscore_list = list([0] * int(contig_length1.iloc[0]))\n",
    "\n",
    "    total_len = int(len(vscore_list))\n",
    "\n",
    "    for index, row in group.iterrows():\n",
    "        S = domain_dictionary[row['vscore_category']]\n",
    "        vscore_list[row['gene_start']:row['gene_stop']] = [S] * (row['gene_stop'] - row['gene_start'])\n",
    "\n",
    "    blocks = int(((len(vscore_list) - window) / sliiiiiide_to_the_right) + 1)\n",
    "    blocks_2 = blocks + 2 #need this otherwise the last (incomplete/little) block will be cut off!\n",
    "\n",
    "    dat_list = []\n",
    "    for i in range(0, blocks_2 * sliiiiiide_to_the_right, sliiiiiide_to_the_right):\n",
    "        score_result = sum(vscore_list[i:i+window])\n",
    "        new_let_list = vscore_list[i:i+window]\n",
    "\n",
    "        if score_result >= 0 :\n",
    "            PF_result = \"pass\"\n",
    "        else:\n",
    "            PF_result = \"fail\"\n",
    "\n",
    "        #counts for later\n",
    "        VirusGene = new_let_list.count(10)\n",
    "        HypotheticalGene = new_let_list.count(5)\n",
    "        BacterialGene = new_let_list.count(-3)\n",
    "        Intergenic = new_let_list.count(0)\n",
    "\n",
    "        #vars for count columns\n",
    "        count = count +1\n",
    "        count_start += sliiiiiide_to_the_right #same as c_s = c_s + siiii...\n",
    "        count_stop = count_start+window\n",
    "\n",
    "        dat_list.append([count, count_start, count_stop, PF_result, score_result, VirusGene,\n",
    "                          HypotheticalGene, BacterialGene, Intergenic])\n",
    "\n",
    "\n",
    "    df_0 = pd.DataFrame(dat_list, columns=['Window', 'Position start', 'Position stop',\n",
    "                                             'Pass/Fail', 'Score', 'VirusGene', 'HypotheticalGene', \n",
    "                                             'Intergenic', 'BacterialGene'])\n",
    "\n",
    "    #Now let's make the smoothed plot\n",
    "\n",
    "    x = df_0['Window']\n",
    "    y = np.array(df_0['Score'])\n",
    "    l = df_0['Window'].count()\n",
    "    df_empty = pd.DataFrame(index=range(l),columns=range(1))\n",
    "    for col in df_empty.columns:\n",
    "        df_empty[col].values[:] = 0\n",
    "\n",
    "    zero=df_empty[0]\n",
    "\n",
    "    def smooth(y, box_pts):\n",
    "        box = np.ones(box_pts)/box_pts\n",
    "        y_smooth = np.convolve(y, box, mode='same')\n",
    "        return y_smooth\n",
    "    smooth_val = 100 #####we can change this if we want!\n",
    "\n",
    "\n",
    "    #statement for handling short sequences (error called if len(y) < smoothing value)\n",
    "    if len(y) <= smooth_val:\n",
    "        smooth_val = int(0.5 * len(y))\n",
    "    else:\n",
    "        smooth_val = int(smooth_val)\n",
    "\n",
    "    smoth = smooth(y,smooth_val)\n",
    "    idx = np.argwhere(np.diff(np.sign(zero - smoth))).flatten()\n",
    "    df_val = pd.DataFrame(zero[idx])\n",
    "    df_val = df_val.reset_index()\n",
    "    #we will save to figures, but first we need to do the validation steps\n",
    "\n",
    "    #This is for validating if region is + or -\n",
    "    df_val.loc[-1] = 1  # adding a row for first position\n",
    "    df_val.index = df_val.index + 1  # shifting index\n",
    "    df_val = df_val.sort_index()\n",
    "\n",
    "    #last position as last row\n",
    "    df_val.sort_values(by=['index']) #need to sort first otherwise +1 belwo will break things\n",
    "    new_list = pd.DataFrame(df_val['index'] + 1) #df['index'][:-1] + 1 #add +1 to all for next position is +/-, except for last position, will throw erre - so it deletes it, we'll add it in later\n",
    "\n",
    "    new_list_2 = new_list['index']\n",
    "\n",
    "    new_y_val = list(smoth[new_list])   #find position y on smooth line\n",
    "\n",
    "    #assigning pos / neg for that +1 position\n",
    "    pos_neg_results = []\n",
    "    for i in new_y_val:\n",
    "        if i > 0:\n",
    "            result = '+'\n",
    "        else:\n",
    "            result = '-'\n",
    "        pos_neg_results.append(result)\n",
    "\n",
    "    #creating dataframe for next steps\n",
    "    df_val.drop(df_val.columns[len(df_val.columns)-1], axis=1, inplace=True) #to delete last column, unnamed so tricky to get rid of (?) this does it tho\n",
    "    df_val['+/- to the right'] = pos_neg_results\n",
    "\n",
    "    #append +/- and start stop coords from original table\n",
    "    df_val.rename(columns={'index': 'Window'}, inplace=True)\n",
    "\n",
    "    df_val['Window']=df_val['Window'].astype(int)\n",
    "    df_0['Window']=df_0['Window'].astype(int)\n",
    "\n",
    "    ## merging\n",
    "    merged_df = df_val.merge(df_0, how = 'inner', on = ['Window'])\n",
    "\n",
    "    merged_df = merged_df.drop(['Pass/Fail','Score','VirusGene','HypotheticalGene','Intergenic','BacterialGene'], axis = 1)\n",
    "    merged_df['Chunk_end'] = 'none'\n",
    "    merged_df['Window midpoint'] = merged_df.iloc[:,[2,3]].median(axis=1)\n",
    "    merged_df['Window midpoint'] = merged_df['Window midpoint'].astype(int)\n",
    "\n",
    "    #df edits to accomodate this:\n",
    "    #we are duplicating the last row of the df to handle a trailing + chunk (w/ no y=0 intercept to close the chunk)\n",
    "    #merged_df = merged_df.append(merged_df[-1:])\n",
    "    merged_df = pd.concat([merged_df, merged_df[-1:]])\n",
    "    #now need to make it read actual last stop position (this os not rounded per window like the other coords)\n",
    "    merged_df = merged_df.replace(merged_df.iloc[-1][3],(total_len+1))\n",
    "\n",
    "    #now let's get the coordinates for the > 0 'chunks'\n",
    "    #iterate over for true hit testing\n",
    "    def pairwise(iterable):\n",
    "        \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "\n",
    "    #this is to define the chunks, accounting for all the ways the graph can look\n",
    "    #note: leading and trailing here mean a chunk at the start or end of the graph that\n",
    "\n",
    "    ## use nearby ORF boundaries to refine cutoffs:\n",
    "\n",
    "    def right_cutoff(position, groupframe):\n",
    "        calced_end =  int(position)\n",
    "        rbisect_pos = bisect.bisect(list(groupframe['gene_stop']), calced_end)\n",
    "        rchunk_cutoff = list(groupframe['gene_stop'])[rbisect_pos-1]\n",
    "        return rchunk_cutoff\n",
    "\n",
    "    def left_cutoff(position, groupframe):\n",
    "        calced_start =  int(position)\n",
    "        lbisect_pos = bisect.bisect(list(groupframe['gene_start']), calced_start)\n",
    "        lchunk_cutoff = list(groupframe['gene_stop'])[lbisect_pos]\n",
    "        return lchunk_cutoff\n",
    "\n",
    "    ddf_list = []\n",
    "\n",
    "    for (i1, row1), (i2, row2) in pairwise(merged_df.iterrows()):\n",
    "        #for a leading chunk\n",
    "        if row1['+/- to the right'] == '+' and \\\n",
    "            row1[\"Position start\"] == 0 and \\\n",
    "            row1[\"Position stop\"] != (total_len + 1):\n",
    "\n",
    "                ddf = [\"Chunk_\" + str(i1), \n",
    "                       row1[\"Position start\"], \n",
    "                       right_cutoff(row2[\"Window midpoint\"], group)]\n",
    "                ddf_list.append(ddf)\n",
    "        #for a contained chunk\n",
    "        if row1['+/- to the right'] == '+' and \\\n",
    "            row1[\"Position start\"] != 0 and \\\n",
    "            row1[\"Position stop\"] != (total_len + 1):\n",
    "\n",
    "                ddf = [\"Chunk_\" + str(i1), \n",
    "                       left_cutoff(row1[\"Window midpoint\"], group), \n",
    "                       right_cutoff(row2[\"Window midpoint\"], group)]\n",
    "                ddf_list.append(ddf)\n",
    "        #3. for a trailing chunk\n",
    "        if row1['+/- to the right'] == '+' and \\\n",
    "            row1[\"Position start\"] != 0 and \\\n",
    "            row1[\"Position stop\"] == (total_len + 1):\n",
    "                \n",
    "                ddf = [\"Chunk_\" + str(i1), \n",
    "                       left_cutoff(row1[\"Window midpoint\"], group), \n",
    "                       row2[\"Position stop\"]]\n",
    "                ddf_list.append(ddf)\n",
    "\n",
    "        #4. for graphs with no leading and no trailing chunk (for graphs with no y = 0 intercept -> this is is\n",
    "        #a differently-defined statemnt below b/c the empty file gets appended w/ stuff above from older files when\n",
    "        #it's in the loop, ALSO the criterion gets fulfilled by contained cunks which means duplicate csv rows for chunks (defined diffrently to specifiy the rules)\n",
    "        if merged_df.iloc[0,1] == '+' and \\\n",
    "            merged_df.iloc[0,2] == 0 and \\\n",
    "            merged_df.iloc[0,3] == (total_len + 1): #if first column last(2nd row) == last -1 then its one chunk\n",
    "                rep_list = [('Chunk_0', '0', (total_len+1))]\n",
    "                ddf_list = rep_list\n",
    "        else:\n",
    "                ddf_list = ddf_list\n",
    "\n",
    "    ## make chunk csv\n",
    "    chunk_df = pd.DataFrame(ddf_list, columns=[\"chunk_number\", \"left_cutoff\", \"right_cutoff\"])\n",
    "\n",
    "    chunk_sum_file = os.path.join(out_dir1, name + \".chunks.tsv\")\n",
    "\n",
    "    chunk_df.to_csv(chunk_sum_file, sep = \"\\t\", index = False)\n",
    "\n",
    "    ###Find optimal location on plot to place hallmark marker\n",
    "    vir_bait_table = group[['gene_start', 'gene_stop', 'Evidence_source']]\\\n",
    "        .query(\"Evidence_source == 'hallmark_hmm'\")\n",
    "    vir_bait_table['mean'] = round(vir_bait_table[['gene_start', 'gene_stop']].mean(axis=1))\n",
    "    vir_bait_table_med_list = list(vir_bait_table['mean'])\n",
    "\n",
    "    points_list = []\n",
    "    for item in vir_bait_table_med_list:\n",
    "        eq = round(((item - 2500) + 50) / 50)\n",
    "        if eq >= len(x):\n",
    "            plot_point = (len(x) - 1) #1 because it can't = len, has to be less\n",
    "        else:\n",
    "            plot_point = eq\n",
    "\n",
    "        points_list.append(plot_point)\n",
    "\n",
    "    new_points_list = [1 if i <=0 else i for i in points_list]\n",
    "\n",
    "    df_0['smoothy'] = smooth(df_0['Score'],100)\n",
    "\n",
    "    #FIGURES\n",
    "    pdf_outname = os.path.join(out_dir1, name + \".figures.pdf\")\n",
    "\n",
    "    figures = PdfPages(pdf_outname)\n",
    "\n",
    "    ## first figure, shows smoothed average\n",
    "\n",
    "    plt.plot(x, y, 'o', ms=0.6)\n",
    "    plt.axhline(0, 0, l)\n",
    "    plt.plot(x, df_0['smoothy'], 'c', lw=2)\n",
    "    ## add halmark gene markers\n",
    "    plt.plot(x, df_0['smoothy'], 'y', markevery = (new_points_list), ms=11.0, marker = '*')\n",
    "    plt.title(\"Viral region calls\")\n",
    "    plt.xlabel('Window')\n",
    "    plt.ylabel('Score')\n",
    "    plt.rc('axes', titlesize=6.8)     # fontsize of the axes title\n",
    "    plt.rc('xtick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=5)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=8)  # fontsize of the figure title\n",
    "    plt.grid(True)\n",
    "    idx = np.argwhere(np.diff(np.sign(zero - smooth(y,100)))).flatten()\n",
    "    plt.plot(x[idx], zero[idx],  'ro', ms=5.0)\n",
    "\n",
    "\n",
    "    plt.plot()\n",
    "    plt.savefig(figures, format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "    ## second figure shows gene type distribution\n",
    "    mycol = ([\"#e7ba52\", \"#637939\", \"#7b4173\", \"#d6616b\"])\n",
    "    df_0[['VirusGene','HypotheticalGene','BacterialGene','Intergenic']].plot(color = mycol)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Window')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Character counts')\n",
    "    plt.rc('axes', titlesize=6.8)     # fontsize of the axes title\n",
    "    plt.rc('xtick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=5)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=8)  # fontsize of the figure title\n",
    "    plt.savefig(figures, format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "    figures.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test bisect\n",
    "\n",
    "for name, group in grouped_df:\n",
    "    group.notnull\n",
    "    prune(name, group, out_dir)\n",
    "\n",
    "\n",
    "#also output contig_gene_df to table file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct2_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
