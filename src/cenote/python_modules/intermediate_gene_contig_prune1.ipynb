{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a gene table with:\n",
    "contig, contig_length, gene_name, gene_start, gene_stop, gene_orient, Evidence_source, evidence_acession, evidence_description, virus_score(or category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "from itertools import tee\n",
    "import csv\n",
    "\n",
    "import statistics\n",
    "from statistics import mean\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat_table = \"/Users/u241374/mike_tisza/sandbox/ct2_test_731c/ct2_tmp/hallmark_contigs_terminal_repeat_summary.tsv\"\n",
    "\n",
    "phan_tab_directory = \"/Users/u241374/mike_tisza/sandbox/ct2_test_731c/ct2_tmp/reORF/phan_split\"\n",
    "\n",
    "prod_tab_directory = \"/Users/u241374/mike_tisza/sandbox/ct2_test_731c/ct2_tmp/reORF/prod_split\"\n",
    "\n",
    "first_pyhmmer_table = \"/Users/u241374/mike_tisza/sandbox/ct2_test_731c/ct2_tmp/reORF_pyhmmer/pyhmmer_report_AAs.tsv\"\n",
    "\n",
    "second_pyhmmer_table = \"/Users/u241374/mike_tisza/sandbox/ct2_test_731c/ct2_tmp/second_reORF_pyhmmer/pyhmmer_report_AAs.tsv\"\n",
    "\n",
    "mmseqs_CDD_table = \"/Users/u241374/mike_tisza/sandbox/ct2_test_731c/ct2_tmp/third_reORF_combined/summary_no2_AAs_vs_CDD.besthit.tsv\"\n",
    "\n",
    "viral_cdds_list = \"/Users/u241374/mike_tisza/cmmr_repos/Cenote-Taker2/viral_cdds_and_pfams_191028.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just_gene_df = pd.DataFrame(columns=[\"gene_name\", \"gene_start\", \"gene_stop\", \"gene_orient\"])\n",
    "try:\n",
    "\n",
    "    phan_files = glob.glob(os.path.join(phan_tab_directory, \"*.bed\"))\n",
    "\n",
    "    df_from_each_phan = (pd.read_csv(phan, sep = \"\\t\", header = None,\n",
    "                                    names = [\"contig\", \"gene_start\", \"gene_stop\", \"gene_name\", \n",
    "                                            \"gene_score\", \"gene_orient\"])\n",
    "                                    for phan in phan_files)\n",
    "    phan_gene_df = pd.concat(df_from_each_phan, ignore_index=True)\n",
    "\n",
    "    phan_gene_df = phan_gene_df.drop(\"gene_score\", axis = 1)\n",
    "\n",
    "except:\n",
    "    print(\"no phanotate tables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no prodigal tables\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "\n",
    "    prod_files = glob.glob(os.path.join(prod_tab_directory, \"*.gff\"))\n",
    "\n",
    "    df_from_each_prod = (pd.read_csv(prod, sep = \"\\t\", header = None, comment='#',\n",
    "                                    names = [\"contig\", \"gene_caller\", \"feature_type\", \"gene_start\", \n",
    "                                            \"gene_stop\", \"score\", \"gene_orient\", \"frame\", \"attribute\"])\n",
    "                                    for prod in prod_files)\n",
    "    prod_gene_df = pd.concat(df_from_each_prod, ignore_index=True)\n",
    "\n",
    "    prod_gene_df = prod_gene_df[[\"contig\", \"gene_start\", \"gene_stop\", \"attribute\", \"gene_orient\"]]\n",
    "except:\n",
    "    print(\"no prodigal tables\")\n",
    "    prod_gene_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phan\n"
     ]
    }
   ],
   "source": [
    "if not phan_gene_df.empty and not prod_gene_df.empty:\n",
    "    print(\"both\")\n",
    "    both_list = [phan_gene_df, prod_gene_df]\n",
    "    just_gene_df = pd.concat(both_list, ignore_index=True)\n",
    "elif not phan_gene_df.empty:\n",
    "    print(\"phan\")\n",
    "    just_gene_df = phan_gene_df\n",
    "elif not prod_gene_df.empty:\n",
    "    print(\"prod\")\n",
    "    just_gene_df = prod_gene_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    length_df = pd.read_csv(repeat_table, sep = \"\\t\")[['contig', 'out_length_contig']]\n",
    "\n",
    "    length_df = length_df.rename(columns={\"out_length_contig\": \"contig_length\"})\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    basal_df = just_gene_df.merge(length_df, on = \"contig\", how = \"left\")\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ORFquery</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ct2_test_731c43597_90(-)</td>\n",
       "      <td>baits_missed.111/PDB:3KDR_A-HK97-Family-Phage-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ct2_test_731c4767_65(+)</td>\n",
       "      <td>tmp1_clust12/Uniprot:R5NKC2_9FIRM-tail-tape-me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ct2_test_731c36689_13(-)</td>\n",
       "      <td>podo_cluster.10/PFAM:PF11134.7--Phage-stabilis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ct2_test_731c2265_7(-)</td>\n",
       "      <td>seeker_phage_cluster19/PDB:3CPE_A-Terminase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ct2_test_731c37702_35(+)</td>\n",
       "      <td>cdd_cluster.55/CDD:PHA02535-terminase-ATPase-s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ORFquery                                             target\n",
       "0  ct2_test_731c43597_90(-)  baits_missed.111/PDB:3KDR_A-HK97-Family-Phage-...\n",
       "1   ct2_test_731c4767_65(+)  tmp1_clust12/Uniprot:R5NKC2_9FIRM-tail-tape-me...\n",
       "2  ct2_test_731c36689_13(-)  podo_cluster.10/PFAM:PF11134.7--Phage-stabilis...\n",
       "3    ct2_test_731c2265_7(-)        seeker_phage_cluster19/PDB:3CPE_A-Terminase\n",
       "4  ct2_test_731c37702_35(+)  cdd_cluster.55/CDD:PHA02535-terminase-ATPase-s..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_pyh_df = pd.read_csv(first_pyhmmer_table, sep = \"\\t\")[['ORFquery', 'target']]\n",
    "\n",
    "first_pyh_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gene_name, Evidence_source, evidence_acession, evidence_description, virus_score(or category)\n",
    "\n",
    "try:\n",
    "    first_pyh_df = pd.read_csv(first_pyhmmer_table, sep = \"\\t\")[['ORFquery', 'target']]\n",
    "\n",
    "\n",
    "    first_pyh_df[\"lpar_pos\"] = first_pyh_df[\"ORFquery\"].str.find(\"(\")\n",
    "\n",
    "    first_pyh_df[\"gene_name\"] = first_pyh_df.apply(lambda x: x[\"ORFquery\"][0:x[\"lpar_pos\"]], axis = 1)\n",
    "\n",
    "    first_pyh_df[\"slash_pos\"] = first_pyh_df[\"target\"].str.find(\"/\")\n",
    "    first_pyh_df[\"fdash_pos\"] = first_pyh_df[\"target\"].str.find(\"-\")\n",
    "\n",
    "\n",
    "    first_pyh_df[\"evidence_acession\"] = first_pyh_df.apply(\n",
    "        lambda x: x[\"target\"][x[\"slash_pos\"]+1:x[\"fdash_pos\"]], axis = 1)\n",
    "\n",
    "    first_pyh_df[\"evidence_description\"] = first_pyh_df.apply(lambda x: x[\"target\"][x[\"fdash_pos\"]+1:], axis = 1)\n",
    "\n",
    "    first_pyh_df = first_pyh_df[['gene_name', 'evidence_acession', 'evidence_description']]\n",
    "\n",
    "    first_pyh_df['Evidence_source'] = 'hallmark_hmm'\n",
    "\n",
    "    first_pyh_df['vscore_category'] = 'common_virus'\n",
    "\n",
    "except:\n",
    "    print(\"nope\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    second_pyh_df = pd.read_csv(second_pyhmmer_table, sep = \"\\t\")[['ORFquery', 'target']]\n",
    "\n",
    "\n",
    "    second_pyh_df[\"lpar_pos\"] = second_pyh_df[\"ORFquery\"].str.find(\"(\")\n",
    "\n",
    "    second_pyh_df[\"gene_name\"] = second_pyh_df.apply(lambda x: x[\"ORFquery\"][0:x[\"lpar_pos\"]], axis = 1)\n",
    "\n",
    "    second_pyh_df[\"slash_pos\"] = second_pyh_df[\"target\"].str.find(\"/\")\n",
    "    second_pyh_df[\"fdash_pos\"] = second_pyh_df[\"target\"].str.find(\"-\")\n",
    "\n",
    "\n",
    "    second_pyh_df[\"evidence_acession\"] = second_pyh_df.apply(\n",
    "        lambda x: x[\"target\"][x[\"slash_pos\"]+1:x[\"fdash_pos\"]], axis = 1)\n",
    "\n",
    "    second_pyh_df[\"evidence_description\"] = second_pyh_df.apply(lambda x: x[\"target\"][x[\"fdash_pos\"]+1:], axis = 1)\n",
    "\n",
    "    second_pyh_df = second_pyh_df[['gene_name', 'evidence_acession', 'evidence_description']]\n",
    "\n",
    "    second_pyh_df['Evidence_source'] = 'common_virus_hmm'\n",
    "\n",
    "    second_pyh_df['vscore_category'] = 'common_virus'\n",
    "\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_name</th>\n",
       "      <th>evidence_acession</th>\n",
       "      <th>evidence_description</th>\n",
       "      <th>Evidence_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ct2_test_731c2983_13</td>\n",
       "      <td>pfam14902</td>\n",
       "      <td>Domain of unknown function (DUF4494). This fam...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ct2_test_731c3157_42</td>\n",
       "      <td>pfam08877</td>\n",
       "      <td>MepB protein. MepB is a functionally uncharact...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ct2_test_731c3157_43</td>\n",
       "      <td>cd01031</td>\n",
       "      <td>ClC chloride channel EriC.  This domain is fou...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ct2_test_731c3157_44</td>\n",
       "      <td>PRK00015</td>\n",
       "      <td>ribonuclease HII; Validated</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ct2_test_731c3157_45</td>\n",
       "      <td>TIGR03596</td>\n",
       "      <td>ribosome biogenesis GTP-binding protein YlqF. ...</td>\n",
       "      <td>mmseqs_cdd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              gene_name evidence_acession  \\\n",
       "0  ct2_test_731c2983_13         pfam14902   \n",
       "1  ct2_test_731c3157_42         pfam08877   \n",
       "2  ct2_test_731c3157_43           cd01031   \n",
       "3  ct2_test_731c3157_44          PRK00015   \n",
       "4  ct2_test_731c3157_45         TIGR03596   \n",
       "\n",
       "                                evidence_description Evidence_source  \n",
       "0  Domain of unknown function (DUF4494). This fam...      mmseqs_cdd  \n",
       "1  MepB protein. MepB is a functionally uncharact...      mmseqs_cdd  \n",
       "2  ClC chloride channel EriC.  This domain is fou...      mmseqs_cdd  \n",
       "3                        ribonuclease HII; Validated      mmseqs_cdd  \n",
       "4  ribosome biogenesis GTP-binding protein YlqF. ...      mmseqs_cdd  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdd_df = pd.read_csv(mmseqs_CDD_table, sep = \"\\t\")[['query', 'target', 'description']]\n",
    "\n",
    "cdd_df[\"lpar_pos\"] = cdd_df[\"query\"].str.find(\"(\")\n",
    "\n",
    "cdd_df[\"gene_name\"] = cdd_df.apply(lambda x: x[\"query\"][0:x[\"lpar_pos\"]], axis = 1)\n",
    "\n",
    "cdd_df = cdd_df[['gene_name', 'target', 'description']]\n",
    "\n",
    "cdd_df = cdd_df.rename(columns={\"target\": \"evidence_acession\", \"description\" : \"evidence_description\"})\n",
    "\n",
    "cdd_df['Evidence_source'] = 'mmseqs_cdd'\n",
    "\n",
    "#cdd_df['vscore_category'] = 'nonviral_gene'\n",
    "\n",
    "#cdd_df['vscore_category'] = np.where(cdd_df['target']\n",
    "#                               .str.contains(\n",
    "#                                   \"PHA0\",\n",
    "#                                   case = False), 'common_virus', cdd_df['vscore_category'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cdd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "virlist_df = pd.read_csv(viral_cdds_list, header = None, names = [\"evidence_acession\"])\n",
    "\n",
    "virlist_df['vscore_category'] = 'common_virus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_cdd_df = cdd_df.merge(virlist_df, on = \"evidence_acession\", how = \"left\")\n",
    "\n",
    "comb_cdd_df['vscore_category'] = np.where(comb_cdd_df['evidence_acession']\n",
    "                               .str.contains(\n",
    "                                   \"PHA0\",\n",
    "                                   case = False), 'common_virus', comb_cdd_df['vscore_category'])\n",
    "\n",
    "comb_cdd_df['vscore_category'] = np.where(comb_cdd_df['vscore_category'].isna(), 'nonviral_gene',\n",
    "                                          comb_cdd_df['vscore_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ann_list = []\n",
    "\n",
    "if not first_pyh_df.empty:\n",
    "    gene_ann_list.append(first_pyh_df)\n",
    "\n",
    "if not second_pyh_df.empty:\n",
    "    gene_ann_list.append(second_pyh_df)\n",
    "\n",
    "if not comb_cdd_df.empty:\n",
    "    gene_ann_list.append(comb_cdd_df)\n",
    "\n",
    "try:\n",
    "    gene_ann_df = pd.concat(gene_ann_list, ignore_index=True)\n",
    "\n",
    "    contig_gene_df = basal_df.merge(gene_ann_df, on = \"gene_name\", how = \"left\")\n",
    "\n",
    "    contig_gene_df['vscore_category'] = np.where(contig_gene_df['vscore_category'].isna(), 'hypothetical_protein',\n",
    "                                          contig_gene_df['vscore_category'])\n",
    "    \n",
    "    contig_gene_df['evidence_description'] = np.where(contig_gene_df['evidence_description'].isna(), 'hypothetical protein',\n",
    "                                          contig_gene_df['evidence_description'])\n",
    "\n",
    "except:\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Window +/- to the right  Position start  Position stop Chunk_end  \\\n",
      "0       1                +               0          12963      none   \n",
      "0       1                +               0          12963      none   \n",
      "\n",
      "   Window midpoint  \n",
      "0             2500  \n",
      "0             2500  \n",
      "   Window +/- to the right  Position start  Position stop Chunk_end  \\\n",
      "0       1                +               0          17547      none   \n",
      "0       1                +               0          17547      none   \n",
      "\n",
      "   Window midpoint  \n",
      "0             2500  \n",
      "0             2500  \n",
      "   Window +/- to the right  Position start  Position stop Chunk_end  \\\n",
      "0       1                +               0          62982      none   \n",
      "0       1                +               0          62982      none   \n",
      "\n",
      "   Window midpoint  \n",
      "0             2500  \n",
      "0             2500  \n",
      "   Window +/- to the right  Position start  Position stop Chunk_end  \\\n",
      "0       1                +               0          19539      none   \n",
      "0       1                +               0          19539      none   \n",
      "\n",
      "   Window midpoint  \n",
      "0             2500  \n",
      "0             2500  \n",
      "   Window +/- to the right  Position start  Position stop Chunk_end  \\\n",
      "0       1                +               0          11548      none   \n",
      "0       1                +               0          11548      none   \n",
      "\n",
      "   Window midpoint  \n",
      "0             2500  \n",
      "0             2500  \n"
     ]
    }
   ],
   "source": [
    "####define scoring scheme and important variables\n",
    "id_list = ['common_virus','hypothetical_protein','nonviral_gene','intergenic']\n",
    "score_list = [10,5,-3,0]\n",
    "\n",
    "keys = id_list\n",
    "values = score_list\n",
    "domain_dictionary = dict(zip(keys,values))\n",
    "\n",
    "threshold = 0\n",
    "window = 5000\n",
    "sliiiiiide_to_the_right = 50\n",
    "\n",
    "\n",
    "grouped_df = contig_gene_df.query(\"contig_length >= 10000\").groupby('contig')\n",
    "\n",
    "for name, group in grouped_df:\n",
    "\n",
    "    count=0\n",
    "    count_start=-sliiiiiide_to_the_right\n",
    "\n",
    "    contig_length1 = group['contig_length'].agg(pd.Series.mode)\n",
    "\n",
    "\n",
    "    vscore_list = list([0] * int(contig_length1.iloc[0]))\n",
    "\n",
    "    total_len = int(len(vscore_list))\n",
    "\n",
    "    for index, row in group.iterrows():\n",
    "        #print(row['vscore_category'], domain_dictionary[row['vscore_category']])\n",
    "        S = domain_dictionary[row['vscore_category']]\n",
    "        vscore_list[row['gene_start']:row['gene_stop']] = [S] * (row['gene_stop'] - row['gene_start'])\n",
    "\n",
    "    #letter_list = [domain_dictionary[k] for k in x] #convert to scores\n",
    "    seq_score_nope = sum(vscore_list)\n",
    "    #avg_score = mean(letter_list)\n",
    "    blocks = int(((len(vscore_list) - window) / sliiiiiide_to_the_right) + 1)\n",
    "    blocks_2 = blocks + 2 #need this otherwise the last (incomplete/little) block will be cut off!\n",
    "    #print(\"you will have \" + str(blocks_2) + \" windows\")\n",
    "\n",
    "    #cols = ['Window', 'Position start', 'Position stop','Pass/Fail', 'Score', 'V_count', 'X_count', 'Z_count', 'Y_count']\n",
    "    #dat = pd.DataFrame(columns = cols)\n",
    "    dat_list = []\n",
    "    for i in range(0, blocks_2 * sliiiiiide_to_the_right, sliiiiiide_to_the_right):\n",
    "        score_result = sum(vscore_list[i:i+window])\n",
    "        new_let_list = vscore_list[i:i+window]\n",
    "\n",
    "        if score_result >= 0 :\n",
    "            PF_result = \"pass\"\n",
    "        else:\n",
    "            PF_result = \"fail\"\n",
    "\n",
    "        #counts for later\n",
    "        V_count = new_let_list.count(10)\n",
    "        X_count = new_let_list.count(5)\n",
    "        Y_count = new_let_list.count(-3)\n",
    "        Z_count = new_let_list.count(0)\n",
    "\n",
    "        #vars for count columns\n",
    "        count = count +1\n",
    "        count_start += sliiiiiide_to_the_right #same as c_s = c_s + siiii...\n",
    "        count_stop = count_start+window\n",
    "\n",
    "        #dat.index.name = 'Window'\n",
    "        #let's plot things!\n",
    "        dat_list.append([count, count_start, count_stop, PF_result, score_result, V_count,\n",
    "                          X_count, Y_count, Z_count])\n",
    "        #dat.index.name = 'Window'\n",
    "\n",
    "    #outname = (str(file.name)+\".tableout.tsv\")\n",
    "\n",
    "    #FIGURES\n",
    "    pdf_outname = os.path.join(\"/Users/u241374/mike_tisza/sandbox/figures_out\", name+\".figures.pdf\")\n",
    "    #Character ocunts plot\n",
    "    #figures = PdfPages(pdf_outname)\n",
    "    df_0 = pd.DataFrame(dat_list, columns=['Window', 'Position start', 'Position stop',\n",
    "                                             'Pass/Fail', 'Score', 'V_count', 'X_count', \n",
    "                                             'Z_count', 'Y_count'])\n",
    "    #dat.to_csv(outname, sep='\\t', index=False)\n",
    "    #MAIN DATAFRAME CREATED, STORED IN df_0\n",
    "\n",
    "    #Now let's make the smoothed plot\n",
    "\n",
    "    #median_0 = df_0['Annotation'].median()\n",
    "    x = df_0['Window']\n",
    "    y = np.array(df_0['Score'])\n",
    "    l = df_0['Window'].count()\n",
    "    df_empty = pd.DataFrame(index=range(l),columns=range(1))\n",
    "    for col in df_empty.columns:\n",
    "        df_empty[col].values[:] = 0\n",
    "\n",
    "    zero=df_empty[0]\n",
    "\n",
    "    def smooth(y, box_pts):\n",
    "        box = np.ones(box_pts)/box_pts\n",
    "        y_smooth = np.convolve(y, box, mode='same')\n",
    "        return y_smooth\n",
    "    #smooth_val == box_plts\n",
    "    smooth_val = 100 #####we can change this if we want!\n",
    "\n",
    "\n",
    "    #statement for handling short sequences (error called if len(y) < smoothing value)\n",
    "    if len(y) <= smooth_val:\n",
    "        smooth_val = int(0.5 * len(y))\n",
    "    else:\n",
    "        smooth_val = int(smooth_val)\n",
    "\n",
    "    smoth = smooth(y,smooth_val)\n",
    "    idx = np.argwhere(np.diff(np.sign(zero - smoth))).flatten()\n",
    "    df = pd.DataFrame(zero[idx])\n",
    "    df = df.reset_index()\n",
    "    #we will save to figures, but first we need to do the validation steps\n",
    "\n",
    "    #This is for validating if region is + or -\n",
    "    df.loc[-1] = 1  # adding a row for first position\n",
    "    df.index = df.index + 1  # shifting index\n",
    "    df = df.sort_index()\n",
    "\n",
    "    #df.iloc[-1] = len(y)\n",
    "\n",
    "    #last position as last row\n",
    "    #print(df['index'])\n",
    "    df.sort_values(by=['index']) #need to sort first otherwise +1 belwo will break things\n",
    "    new_list = pd.DataFrame(df['index'] + 1) #df['index'][:-1] + 1 #add +1 to all for next position is +/-, except for last position, will throw erre - so it deletes it, we'll add it in later\n",
    "    #print(new_list)\n",
    "\n",
    "    #the_val_to_add = df.iloc[-1] - 1\n",
    "\n",
    "    #new_list = new_list.append(df.iloc[-1] - 1) #beacuse of +1 transformation few lines above\n",
    "    new_list_2 = new_list['index']\n",
    "\n",
    "    #new_list = new_list.append(last_val_to_append, ignore_index=True)\n",
    "\n",
    "    new_y_val = list(smoth[new_list])   #find position y on smooth line\n",
    "\n",
    "\n",
    "    #assigning pos / neg for that +1 position\n",
    "    pos_neg_results = []\n",
    "    for i in new_y_val:\n",
    "        if i > 0:\n",
    "            result = '+'\n",
    "        else:\n",
    "            result = '-'\n",
    "        pos_neg_results.append(result)\n",
    "\n",
    "    #pos_neg_results.append('N/A') #the last value needs this - not anymore\n",
    "\n",
    "    #print(pos_neg_results)\n",
    "    #creating dataframe for next steps\n",
    "    df.drop(df.columns[len(df.columns)-1], axis=1, inplace=True) #to delete last column, unnamed so tricky to get rid of (?) this does it tho\n",
    "    df['+/- to the right'] = pos_neg_results\n",
    "    #print(df['+/- to the right'])\n",
    "    #append +/- and start stop coords from original table\n",
    "    df.rename(columns={'index': 'Window'}, inplace=True)\n",
    "\n",
    "    df['Window']=df['Window'].astype(int)\n",
    "    df_0['Window']=df_0['Window'].astype(int)\n",
    "\n",
    "    merged_df = df.merge(df_0, how = 'inner', on = ['Window'])\n",
    "\n",
    "    merged_df = merged_df.drop(['Pass/Fail','Score','V_count','X_count','Z_count','Y_count'], axis = 1)\n",
    "    merged_df['Chunk_end'] = 'none'\n",
    "    merged_df['Window midpoint'] = merged_df.iloc[:,[2,3]].median(axis=1)\n",
    "    merged_df['Window midpoint'] = merged_df['Window midpoint'].astype(int)\n",
    "\n",
    "    #df edits to accomodate this:\n",
    "    #we are duplicating the last row of the df to handle a trailing + chunk (w/ no y=0 intercept to close the chunk)\n",
    "    #merged_df = merged_df.append(merged_df[-1:])\n",
    "    merged_df = pd.concat([merged_df, merged_df[-1:]])\n",
    "    #now need to make it read actual last stop position (this os not rounded per window like the other coords)\n",
    "    merged_df = merged_df.replace(merged_df.iloc[-1][3],(total_len+1))\n",
    "    print(merged_df)\n",
    "\n",
    "    #now let's get the coordinates for the > 0 'chunks'\n",
    "\n",
    "    #iterate over for true hit testing\n",
    "    def pairwise(iterable):\n",
    "        \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "        a, b = tee(iterable)\n",
    "        next(b, None)\n",
    "        return zip(a, b)\n",
    "\n",
    "\n",
    "\n",
    "    #this is to define the chunks, accounting for all the ways the graph can look\n",
    "    #note: leading and trailing here mean a chunk at the start or end of the graph that\n",
    "    ddf_list = []\n",
    "\n",
    "    for (i1, row1), (i2, row2) in pairwise(merged_df.iterrows()):\n",
    "        #for a leading chunk\n",
    "        if row1['+/- to the right'] == '+' and \\\n",
    "            row1[\"Position start\"] == 0 and \\\n",
    "            row1[\"Position stop\"] != (total_len + 1):\n",
    "                ddf = [\"Chunk_\" + str(i1), row1[\"Position start\"], row2[\"Window midpoint\"]]\n",
    "                ddf_list.append(ddf)\n",
    "        #for a contained chunk\n",
    "        if row1['+/- to the right'] == '+' and \\\n",
    "            row1[\"Position start\"] != 0 and \\\n",
    "            row1[\"Position stop\"] != (total_len + 1):\n",
    "                ddf = [\"Chunk_\" + str(i1), row1[\"Window midpoint\"], row2[\"Window midpoint\"]]\n",
    "                ddf_list.append(ddf)\n",
    "        #3. for a trailing chunk\n",
    "        if row1['+/- to the right'] == '+' and \\\n",
    "            row1[\"Position start\"] != 0 and \\\n",
    "            row1[\"Position stop\"] == (total_len + 1): #old = merged_df.iloc[0,3]\n",
    "                ddf = [\"Chunk_\" + str(i1), row1[\"Window midpoint\"], row2[\"Position stop\"]]\n",
    "                ddf_list.append(ddf)\n",
    "\n",
    "        #4. for graphs with no leading and no trailing chunk (for graphs with no y = 0 intercept -> this is is\n",
    "        #a differently-defined statemnt below b/c the empty file gets appended w/ stuff above from older files when\n",
    "        #it's in the loop, ALSO the criterion gets fulfilled by contained cunks which means duplicate csv rows for chunks (defined diffrently to specifiy the rules)\n",
    "        if merged_df.iloc[0,1] == '+' and \\\n",
    "            merged_df.iloc[0,2] == 0 and \\\n",
    "            merged_df.iloc[0,3] == (total_len + 1): #if first column last(2nd row) == last -1 then its one chunk\n",
    "                rep_list = [('Chunk_0', '0', (total_len+1))]\n",
    "                ddf_list = rep_list\n",
    "        else:\n",
    "                ddf_list = ddf_list\n",
    "\n",
    "    #print(merged_df)\n",
    "    #print(ddf_list)\n",
    "\n",
    "    #make chunk csv\n",
    "    chunk_df = pd.DataFrame(ddf_list)\n",
    "    #this_name = str(file.name+\"_chunk_coordinates.csv\") #used to be fna_name\n",
    "\n",
    "    chunk_sum_file = os.path.join(\"/Users/u241374/mike_tisza/sandbox/figures_out\", name+\".chunks.tsv\")\n",
    "\n",
    "    chunk_df.to_csv(chunk_sum_file, sep = \"\\t\", index = False)\n",
    "    ###Find optimal location on plot to place validation marker\n",
    "    #read in virus table\n",
    "\n",
    "    #file_name_just_stem = file.name[:-4]\n",
    "\n",
    "    #vir_bait_table = str(actual_file_name_temp+'.VIRUS_BAIT_TABLE.txt')\n",
    "    #with open(vir_bait_table, 'r') as csvfile:\n",
    "    #    reader = csv.reader(csvfile, delimiter='\\t')\n",
    "    #    lines = list(reader)\n",
    "\n",
    "    #vir_bait_table = pd.DataFrame(lines)\n",
    "    #vir_bait_table['median'] = round(vir_bait_table[[1,2]].median(axis=1))\n",
    "    #vir_bait_table_med_list = list(vir_bait_table['median'])\n",
    "    #print(vir_bait_table_med_list)\n",
    "\n",
    "\n",
    "    vir_bait_table = group[['gene_start', 'gene_stop', 'Evidence_source']]\\\n",
    "        .query(\"Evidence_source == 'hallmark_hmm'\")\n",
    "    vir_bait_table['mean'] = round(vir_bait_table[['gene_start', 'gene_stop']].mean(axis=1))\n",
    "    vir_bait_table_med_list = list(vir_bait_table['mean'])\n",
    "    #vir_bait_table_med_list = list(round(vir_bait_table[[1,2]].median(axis=1)))\n",
    "\n",
    "    points_list = []\n",
    "    for item in vir_bait_table_med_list:\n",
    "        eq = round(((item - 2500) + 50) / 50)\n",
    "        if eq >= len(x):\n",
    "            plot_point = (len(x) - 1) #1 because it can't = len, has to be less\n",
    "        else:\n",
    "            plot_point = eq\n",
    "        #plot_point = round(((item - 2500) + 50) / 50) #this must stay at = window length (not half like we had talked about, it makes illogical values...basically if the coordinate is towards the end, applying a window 'inbetween' can be out of bounds)\n",
    "        points_list.append(plot_point)\n",
    "\n",
    "    new_points_list = [1 if i <=0 else i for i in points_list]\n",
    "\n",
    "    #print(points_list) #each item represents/is the best/closet window that captures the viral hallmark region\n",
    "    df_0['smoothy'] = smooth(df_0['Score'],100)\n",
    "    zero=df_empty[0]\n",
    "\n",
    "    figures = PdfPages(pdf_outname)\n",
    "\n",
    "    x2 = (points_list)\n",
    "    plt.plot(x, y, 'o', ms=0.6)\n",
    "    plt.axhline(0, 0, l)\n",
    "    #plt.plot(x, smooth(y,3), 'r-', lw=2)\n",
    "    #p = smooth(y,100)\n",
    "    plt.plot(x, df_0['smoothy'], 'c', lw=2)\n",
    "    plt.plot(x, df_0['smoothy'], 'y', markevery = (new_points_list), ms=11.0, marker = '*')\n",
    "    plt.title(\"Viral region calls\")\n",
    "    plt.xlabel('Window')\n",
    "    plt.ylabel('Score')\n",
    "    plt.rc('axes', titlesize=6.8)     # fontsize of the axes title\n",
    "    plt.rc('xtick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=5)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=8)  # fontsize of the figure title\n",
    "    plt.grid(True)\n",
    "    idx = np.argwhere(np.diff(np.sign(zero - smooth(y,100)))).flatten()\n",
    "    plt.plot(x[idx], zero[idx],  'ro', ms=5.0)\n",
    "    #plt.plot(x[idx], zero[idx], markevery= (points_list), ms=9.0, marker = 'X', color = 'y')\n",
    "    #plt.plot()\n",
    "    df = pd.DataFrame(zero[idx])\n",
    "    plt.plot()\n",
    "    plt.savefig(figures, format='pdf')\n",
    "    plt.close()\n",
    "    df = df.reset_index()\n",
    "    #print(df)\n",
    "\n",
    "    mycol = ([\"#e7ba52\", \"#637939\", \"#7b4173\", \"#d6616b\"])\n",
    "    df_0[['V_count','X_count','Y_count','Z_count']].plot(color = mycol) #same = dat.plot(y=['X_count','N_count','R_count','V_count']) , plt.show()        plt.grid(True)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Window')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Character counts')\n",
    "    plt.rc('axes', titlesize=6.8)     # fontsize of the axes title\n",
    "    plt.rc('xtick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('ytick', labelsize=5)    # fontsize of the tick labels\n",
    "    plt.rc('legend', fontsize=5)    # legend fontsize\n",
    "    plt.rc('figure', titlesize=8)  # fontsize of the figure title\n",
    "    plt.savefig(figures, format='pdf')\n",
    "    plt.close()\n",
    "\n",
    "    figures.close()\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct2_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
